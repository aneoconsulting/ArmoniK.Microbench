name: Microbenchmarks on Release

on:
  release:
    types: [published]
  workflow_dispatch:
    inputs:
      release_tag:
        description: 'Release tag to benchmark'
        required: true
        default: 'main'
      armonik_core_repository:
        description: 'ArmoniK.Core repository'
        required: true
        default: 'AncientPatata/ArmoniK.Core'
      microbench_ref:
        description: 'ArmoniK.Microbench ref to checkout'
        required: false
        default: ''
  repository_dispatch:
    types: [trigger-microbenchmarks]

env:
  AWS_DEFAULT_REGION: us-east-1
  TERRAFORM_VERSION: latest
  PYTHON_VERSION: "3.12"
  REMOTE_TFSTATE_BUCKET: "armonik-microbench-backend-tfstate"
  RUN_ID_UNIQUE: ${{ vars.LOCAL_RUN_ID || github.run_id }}

jobs:
  microbenchmarks:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    environment: microbench 
      
    steps:
      - name: Set release tag
        id: set_tag
        run: |
          if [ "${{ github.event_name }}" = "repository_dispatch" ]; then
            echo "release_tag=${{ github.event.client_payload.release_tag }}" >> $GITHUB_OUTPUT
          elif [ "${{ github.event_name }}" = "release" ]; then
            echo "release_tag=${{ github.event.release.tag_name }}" >> $GITHUB_OUTPUT
          else
            echo "release_tag=${{ github.event.inputs.release_tag }}" >> $GITHUB_OUTPUT
          fi
      - name: Set ArmoniK.Core repository
        id: set_core_repo
        run: |
          if [ "${{ github.event_name }}" = "repository_dispatch" ]; then
            echo "armonik_core_repository=${{ github.event.client_payload.armonik_core_repository }}" >> $GITHUB_OUTPUT
          else
            echo "armonik_core_repository=${{ github.event.inputs.armonik_core_repository || 'aneoconsulting/ArmoniK.Core' }}" >> $GITHUB_OUTPUT
          fi

      - name: Set microbench ref
        id: set_microbench_ref
        run: |
          if [ "${{ github.event_name }}" = "repository_dispatch" ]; then
            REF="${{ github.event.client_payload.microbench_ref }}"
          else
            REF="${{ github.event.inputs.microbench_ref }}"
          fi
          # Default to the current SHA (or main for releases)
          if [ -z "$REF" ]; then
            REF="${{ github.sha }}"
          fi
          echo "ref=$REF" >> $GITHUB_OUTPUT

      - name: Checkout ArmoniK.Microbench
        uses: actions/checkout@v6
        with:
          ref: ${{ steps.set_microbench_ref.outputs.ref }}
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Install the latest version of uv
        uses: astral-sh/setup-uv@v6
        with:
          version: "latest"

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

        # TODO: Switch to OIDC later          
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4.1.0
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: ${{ env.AWS_DEFAULT_REGION }}

      - name: Clone ArmoniK.Core
        uses: actions/checkout@v6
        with:
          repository: ${{steps.set_core_repo.outputs.armonik_core_repository}}
          ref: ${{ steps.set_tag.outputs.release_tag }}
          path: ArmoniK.Core
      - name: Copy parameters.tfvars
        run: |
          PARAMS_FILE="ArmoniK.Core/microbenchmarks/parameters.tfvars"
          if [ ! -f "$PARAMS_FILE" ]; then
            echo "::error::parameters.tfvars not found at $PARAMS_FILE"
            echo "Expected file structure: ArmoniK.Core/microbenchmarks/parameters.tfvars"
            exit 1
          fi
          cp "$PARAMS_FILE" infrastructure/

      - name: Initialize and Deploy Infrastructure
        id: deploy
        run: |
          cd infrastructure

          cat > backend.tf <<EOF
          terraform {
            backend "s3" {
              bucket = "$REMOTE_TFSTATE_BUCKET"
              key    = "microbench/${{ env.RUN_ID_UNIQUE }}/terraform.tfstate"
              region = "us-east-1"
            }
          }
          EOF

          # Initialize Terraform
          terraform init -backend-config=bucket=$REMOTE_TFSTATE_BUCKET -backend-config=key=microbench/${{ env.RUN_ID_UNIQUE }}/terraform.tfstate
          
          # Apply infrastructure with the parameters file
          terraform apply -var-file="parameters.tfvars" -var="prefix=microbench-${{ env.RUN_ID_UNIQUE }}" -auto-approve

          echo "terraform_state_key=microbench/${{ env.RUN_ID_UNIQUE }}/terraform.tfstate" >> $GITHUB_OUTPUT
          echo "runner_host=$(terraform output -raw benchmark_instance_ip)" >> $GITHUB_OUTPUT

          # For the key, write it to a file since it's sensitive
          terraform output -raw benchmark_private_key_pem > /tmp/runner_key.pem
          chmod 600 /tmp/runner_key.pem
          echo "runner_key_path=/tmp/runner_key.pem" >> $GITHUB_OUTPUT

      - name: Wait for Runner Instance
        run: |
          RUNNER_HOST="${{ steps.deploy.outputs.runner_host }}"
          RUNNER_KEY="${{ steps.deploy.outputs.runner_key_path }}"
          echo "Waiting for runner instance to be reachable..."
          MAX_ATTEMPTS=30
          ATTEMPT=0
          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 \
              -i "$RUNNER_KEY" ubuntu@"$RUNNER_HOST" "echo ready" 2>/dev/null; then
              echo "Runner is reachable after ~$((ATTEMPT * 10))s"
              break
            fi
            ATTEMPT=$((ATTEMPT + 1))
            echo "Attempt $ATTEMPT/$MAX_ATTEMPTS - waiting 10s..."
            sleep 10
          done
          if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
            echo "::error::Runner instance not reachable after $((MAX_ATTEMPTS * 10))s"
            exit 1
          fi


      - name: Create Microbenchmark Study
        id: create_study
        run: |
          # cd ArmoniK.Microbench
          
          # Get release version
          RELEASE_VERSION="${{ steps.set_tag.outputs.release_tag }}"
          STUDY_NAME="release-${RELEASE_VERSION}-$(date +%Y%m%d-%H%M%S)"
          
          echo "study_name=${STUDY_NAME}" >> $GITHUB_OUTPUT
          
          # Create study using the microbenchmark CLI
          uv run microbenchmark.py study create "${STUDY_NAME}" \
            --core-version "${RELEASE_VERSION}" \
            --runner-version "latest" 

      - name: Run Microbenchmarks
        run: |
          STUDY_NAME="${{ steps.create_study.outputs.study_name }}"
          RUNNER_HOST="${{ steps.deploy.outputs.runner_host }}"
          RUNNER_KEY="${{ steps.deploy.outputs.runner_key_path }}"
          CONFIG_LIST="${{ steps.find_configs.outputs.config_list }}"
          
          TOTAL=0
          FAILED=0
          FAILED_CONFIGS=""
          
          run_benchmark() {
            local config="$1"
            TOTAL=$((TOTAL + 1))
            echo "::group::Benchmark: $config"
            if ! uv run microbenchmark.py study run "$STUDY_NAME" \
                --repo-url "https://github.com/AncientPatata/ArmoniK.Microbench.git" \
                --runner "./infrastructure/benchmark_configs/runners/benchmark_runner.json" \
                --config "$config" \
                --s3-bucket "armonik-microbench-results" \
                --profile "default"; then
              FAILED=$((FAILED + 1))
              FAILED_CONFIGS="$FAILED_CONFIGS\n  - $config"
              echo "::warning::Benchmark failed for $config"
            fi
            echo "::endgroup::"
          }
          
          if [ -n "$CONFIG_LIST" ]; then
            for config in $CONFIG_LIST; do
              run_benchmark "$config"
            done
          elif [ -d "./infrastructure/benchmark_configs" ]; then
            for config in ./infrastructure/benchmark_configs/*.json; do
              [ -f "$config" ] && run_benchmark "$config"
            done
          else
            echo "::error::No benchmark configurations found"
            exit 1
          fi
          
          echo "## Benchmark Execution Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Total:** $TOTAL" >> $GITHUB_STEP_SUMMARY
          echo "- **Passed:** $((TOTAL - FAILED))" >> $GITHUB_STEP_SUMMARY
          echo "- **Failed:** $FAILED" >> $GITHUB_STEP_SUMMARY
          
          if [ $FAILED -gt 0 ]; then
            echo -e "::warning::$FAILED/$TOTAL benchmarks failed:$FAILED_CONFIGS"
          fi
          if [ $FAILED -eq $TOTAL ] && [ $TOTAL -gt 0 ]; then
            echo "::error::All benchmarks failed"
            exit 1
          fi

      - name: Retrieve Results
        if: always()
        run: |
          STUDY_NAME="${{ steps.create_study.outputs.study_name }}"
          if [ -z "$STUDY_NAME" ]; then
            echo "::warning::No study name available, skipping sync"
            exit 0
          fi
          
          if ! uv run microbenchmark.py study sync "$STUDY_NAME" \
              --output-dir "./results" --no-profile; then
            echo "::warning::Failed to sync results from S3"
          fi

      - name: Upload Results as Artifacts
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: microbench-results-${{ steps.set_tag.outputs.release_tag }}
          path: |
            results/
            studies/
          retention-days: 30

      - name: Generate Results Summary
        if: always()
        run: |
          STUDY_NAME="${{ steps.create_study.outputs.study_name }}"
          
          echo "## ðŸš€ Microbenchmark Results for Release ${{ steps.set_tag.outputs.release_tag }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Study Name:** $STUDY_NAME" >> $GITHUB_STEP_SUMMARY
          echo "**Core Version:** ${{ steps.set_tag.outputs.release_tag }}" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "studies/${STUDY_NAME}.json" ]; then
            echo "### Study Details" >> $GITHUB_STEP_SUMMARY
            python3 << PYEOF >> $GITHUB_STEP_SUMMARY
          import json
          try:
              with open('studies/${STUDY_NAME}.json', 'r') as f:
                  study = json.load(f)
              if 'runs' in study and study['runs']:
                  run = study['runs'][-1]
                  benchmarks = run.get('benchmarks', {})
                  total = len(benchmarks)
                  successful = sum(1 for b in benchmarks.values() if b.get('status') == 'success')
                  print(f'- **Total Benchmarks:** {total}')
                  print(f'- **Successful:** {successful}')
                  print(f'- **Failed:** {total - successful}')
                  if benchmarks:
                      print('- **Benchmark Names:**')
                      for name in benchmarks.keys():
                          status = benchmarks[name].get('status', 'unknown')
                          print(f'  - {name}: {status}')
          except Exception as e:
              print(f'Could not parse study results: {e}')
          PYEOF
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Results have been uploaded as workflow artifacts and stored in S3." >> $GITHUB_STEP_SUMMARY

      - name: Destroy Infrastructure
        if: always()
        run: |
          cd infrastructure
          
          terraform init \
            -backend-config=bucket=$REMOTE_TFSTATE_BUCKET \
            -backend-config=key=microbench/${{ env.RUN_ID_UNIQUE }}/terraform.tfstate
          
          MAX_RETRIES=1 # TODO: Setting it to 1 for now because I always destroy mainly and the failure is likely credential expiration
          for i in $(seq 1 $MAX_RETRIES); do
            echo "Destroy attempt $i/$MAX_RETRIES..."
            if terraform destroy \
                -var-file="parameters.tfvars" \
                -var="prefix=microbench-${{ env.RUN_ID_UNIQUE }}" \
                -auto-approve; then
              echo "Infrastructure destroyed successfully"
              break
            fi
            
            if [ $i -eq $MAX_RETRIES ]; then
              echo "::error::Failed to destroy infrastructure after $MAX_RETRIES attempts. MANUAL CLEANUP REQUIRED."
              echo "## :rotating_light: Infrastructure Cleanup Failed" >> $GITHUB_STEP_SUMMARY
              echo "Terraform state key: microbench/${{ env.RUN_ID_UNIQUE }}/terraform.tfstate" >> $GITHUB_STEP_SUMMARY
              echo "Run \`terraform destroy\` manually." >> $GITHUB_STEP_SUMMARY
              exit 1
            fi
            
            echo "Retrying in 30s..."
            sleep 30
          done

      - name: Cleanup
        if: always()
        run: |
          # Clean up any sensitive files
          find . -name "*.pem" -type f -delete
          find . -name "*key*" -type f -delete
          find . -name "terraform.tfstate*" -type f -delete
