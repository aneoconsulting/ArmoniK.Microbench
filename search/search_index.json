{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ArmoniK Microbenchmarks","text":"<p>ArmoniK.Microbench is a performance benchmarking framework for ArmoniK components. It measures the throughput and latency of the various storage and messaging adapters that ArmoniK supports, using BenchmarkDotNet under the hood.</p>"},{"location":"#what-does-it-benchmark","title":"What does it benchmark?","text":"<p>The framework currently supports benchmarking the following ArmoniK adapters:</p>"},{"location":"#object-storage-adapters","title":"Object Storage Adapters","text":"Adapter Infrastructure Operations Measured Redis AWS ElastiCache Add, Get, Delete, GetSize S3 AWS S3 Add, Get, Delete, GetSize LocalStorage Local filesystem (or EFS) Add, Get, Delete, GetSize"},{"location":"#queue-adapters","title":"Queue Adapters","text":"Adapter Infrastructure Operations Measured SQS AWS SQS Push, Pull+Ack, Pull+Nack, PushThenPull, PushThenPullPerRunner RabbitMQ AmazonMQ or EC2 Push, Pull+Ack, Pull+Nack, PushThenPull, PushThenPullPerRunner ActiveMQ AmazonMQ Push, Pull+Ack, Pull+Nack, PushThenPull, PushThenPullPerRunner <p>All benchmarks run with configurable concurrency levels to simulate realistic multi-worker scenarios.</p>"},{"location":"#how-it-works","title":"How it works","text":"<p>The project follows a three-phase workflow:</p> <pre><code>1. Deploy        2. Benchmark       3. Collect\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Terraform \u2502\u2500\u2500\u2500&gt;\u2502 BenchmoniK   \u2502\u2500\u2500\u2500&gt;\u2502 Results \u2192 S3 \u2502\n\u2502 (infra)   \u2502    \u2502 (.NET runner)\u2502    \u2502 + study JSON \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <ol> <li> <p>Deploy infrastructure -- Terraform provisions the AWS resources needed for the benchmarks you want to run (EC2 runner instance, Redis cluster, S3 bucket, SQS queues, etc.). Each module outputs a JSON config file consumed by the benchmark runner.</p> </li> <li> <p>Run benchmarks -- The Microbenchmark CLI (or the CI workflow) SSHs into the runner instance, uploads the config files, and executes the BenchmoniK .NET tool. BenchmarkDotNet runs the benchmarks with statistical rigor (multiple iterations, warmup, memory diagnostics).</p> </li> <li> <p>Collect results -- Benchmark artifacts (BenchmarkDotNet reports, logs) are uploaded to S3. The CLI's study system tracks metadata, config snapshots, and S3 result locations in a local JSON file.</p> </li> </ol>"},{"location":"#project-components","title":"Project Components","text":"Component Language Description Microbenchmark CLI (<code>microbenchmark.py</code>) Python Orchestrates the full workflow: study management, SSH into runners, benchmark execution, result syncing BenchmoniK (<code>benchmark_runner/</code>) C# (.NET 10) The actual benchmark runner. Takes a config file, instantiates the appropriate ArmoniK adapter, and runs BenchmarkDotNet benchmarks Infrastructure (<code>infrastructure/</code>) HCL (Terraform) Modular Terraform code for deploying all AWS resources needed for benchmarking Visualization (<code>microbench-analysis/</code>) Python Analysis and visualization tool for benchmark results (WIP)"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started -- Prerequisites, setup, and running your first benchmark</li> <li>Studies -- How the study system works and CLI reference</li> <li>Infrastructure -- Terraform modules and configuration reference</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+ and uv (the CLI is a single-file script with inline deps, run via <code>uv run</code>)</li> <li>Terraform &gt;= 1.0 (tested with 1.x)</li> <li>AWS CLI configured with credentials that can create EC2, ElastiCache, S3, SQS, AmazonMQ, VPC, and IAM resources</li> <li>.NET 10 SDK (only needed if building the benchmark runner locally; the runner EC2 instance installs it automatically)</li> <li>An AWS account with sufficient permissions and service quotas for the instance types you plan to use</li> </ul>"},{"location":"getting-started/#project-structure","title":"Project Structure","text":"<p>The ArmoniK Microbenchmark project is split into 4 main components:</p> <ul> <li> <p>Microbenchmark CLI</p> <p>The main tool you interact with. <code>microbenchmark.py</code> is a Python CLI (powered by Click and Fabric) that orchestrates everything: creating studies, SSHing into the runner instance, executing benchmarks, and syncing results from S3.</p> <p> CLI Reference</p> </li> <li> <p>Infrastructure</p> <p>Modular Terraform code that deploys an EC2 benchmark runner and the AWS-managed services (ElastiCache, S3, SQS, AmazonMQ) needed for each benchmark. You choose which modules to enable via a <code>parameters.tfvars</code> file.</p> <p> Infrastructure Reference</p> </li> <li> <p>Benchmark Runner (BenchmoniK)</p> <p>A .NET 10 CLI tool that takes a benchmark configuration JSON file and uses BenchmarkDotNet to execute the corresponding microbenchmark against the real ArmoniK adapter code from ArmoniK.Core.</p> </li> <li> <p>Visualization Tool</p> <p>A Streamlit-based analysis tool that takes study results and generates interactive visualizations for comparing adapter performance. (Work in progress)</p> </li> </ul>"},{"location":"getting-started/#setup","title":"Setup","text":""},{"location":"getting-started/#1-clone-the-repository","title":"1. Clone the repository","text":"<pre><code>git clone --recurse-submodules https://github.com/aneoconsulting/ArmoniK.Microbench.git\ncd ArmoniK.Microbench\n</code></pre> <p>The <code>--recurse-submodules</code> flag is important -- it pulls in ArmoniK.Core which is referenced as a submodule and is needed for building the benchmark runner.</p>"},{"location":"getting-started/#2-configure-aws-credentials","title":"2. Configure AWS credentials","text":"<p>Make sure your AWS credentials are configured. The project supports both named profiles and environment variables:</p> <pre><code># Option A: Named profile\naws configure --profile my-profile\n\n# Option B: Environment variables\nexport AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre>"},{"location":"getting-started/#3-create-a-parameters-file","title":"3. Create a parameters file","text":"<p>Create <code>infrastructure/parameters.tfvars</code> to specify which benchmarks to deploy. Set a variable to <code>{}</code> (or an object with overrides) to enable it, or leave it out / set it to <code>null</code> to skip it.</p> <pre><code># Required\nprefix = \"my-bench\"\n\n# Runner configuration (always deployed)\nbenchmark_runner = {\n  instance_type = \"c7a.8xlarge\"\n}\n\n# Enable the benchmarks you want (set to null or omit to skip)\nredis_benchmark = {\n  instance_type = \"cache.m5.xlarge\"\n}\n\ns3_benchmark = {}\n\nlocalstorage_benchmark = {\n  fs_path = \"/tmp/localstorage_benchtemp\"\n}\n\nsqs_benchmark = {}\n\n# RabbitMQ via AmazonMQ managed service\nrabbitmq_amq_benchmark = {\n  instance_type     = \"mq.m5.4xlarge\"\n  username_override = \"rabbitmqbench\"\n  password_override = \"rabbitmqbench\"\n}\n\n# RabbitMQ on a standalone EC2 instance\nrabbitmq_ec2_benchmark = {\n  instance_type = \"m5.4xlarge\"\n}\n\n# ActiveMQ via AmazonMQ managed service\nactivemq_benchmark = {\n  instance_type     = \"mq.m5.4xlarge\"\n  username_override = \"activemqbench\"\n  password_override = \"activemqbench\"\n}\n\n# EFS (used as a network filesystem for the LocalStorage adapter)\nefs_benchmark = {}\n</code></pre>"},{"location":"getting-started/#4-deploy-infrastructure","title":"4. Deploy infrastructure","text":"<pre><code>cd infrastructure\nterraform init\nterraform apply -var-file=\"parameters.tfvars\"\n</code></pre> <p>Terraform will:</p> <ul> <li>Create a VPC with public subnets</li> <li>Launch an EC2 runner instance (with .NET SDK, AWS CLI pre-installed)</li> <li>Deploy the enabled services (Redis, S3, SQS, etc.)</li> <li>Generate SSH keys and benchmark config JSON files under <code>infrastructure/benchmark_configs/</code></li> </ul> <p>Note</p> <p>Deployment can take 5-15 minutes depending on which modules you enable. AmazonMQ brokers in particular take several minutes to provision.</p>"},{"location":"getting-started/#5-run-your-first-benchmark","title":"5. Run your first benchmark","text":"<p>Once infrastructure is deployed, you can either use the study workflow (recommended) or the lower-level runner commands.</p>"},{"location":"getting-started/#option-a-study-workflow-recommended","title":"Option A: Study workflow (recommended)","text":"<p>The study workflow handles init, build, benchmark execution, and result upload in one command:</p> <pre><code># Create a study\nuv run microbenchmark.py study create \"my-first-study\" \\\n  --core-version \"0.25.1\"\n\n# Run benchmarks (this SSHs into the runner, clones the repo, builds Core, and runs benchmarks)\nuv run microbenchmark.py study run \"my-first-study\" \\\n  --directory \"./infrastructure/benchmark_configs\" \\\n  --s3-bucket \"armonik-microbench-results\" \\\n  --profile \"default\"\n\n# Download results locally\nuv run microbenchmark.py study sync \"my-first-study\" \\\n  --output-dir \"./results\"\n</code></pre> <p>See Studies for the full reference.</p>"},{"location":"getting-started/#option-b-runner-commands-manual-step-by-step","title":"Option B: Runner commands (manual step-by-step)","text":"<pre><code># Initialize the runner (clone repo, restore dependencies)\nuv run microbenchmark.py runner init\n\n# Build ArmoniK.Core on the runner\nuv run microbenchmark.py runner build-core --repo-branch \"0.25.1\"\n\n# Run a single benchmark\nuv run microbenchmark.py runner bench --config-file \"./infrastructure/benchmark_configs/redis.json\"\n\n# Retrieve results\nuv run microbenchmark.py runner retrieve-results \\\n  --s3-bucket \"armonik-microbench-results\"\n</code></pre>"},{"location":"getting-started/#6-tear-down-infrastructure","title":"6. Tear down infrastructure","text":"<p>When you're done benchmarking, destroy the infrastructure to avoid ongoing costs:</p> <pre><code>cd infrastructure\nterraform destroy -var-file=\"parameters.tfvars\"\n</code></pre> <p>Warning</p> <p>Always destroy infrastructure when you're done. The EC2 runner and managed services (especially ElastiCache and AmazonMQ) incur costs while running.</p>"},{"location":"getting-started/#cicd-github-actions","title":"CI/CD (GitHub Actions)","text":"<p>The repository includes a GitHub Actions workflow (<code>.github/workflows/run-microbenchmarks.yml</code>) that automates the full lifecycle:</p> <ol> <li>Checkout code and ArmoniK.Core at the specified release tag</li> <li>Deploy infrastructure via Terraform (state stored in S3)</li> <li>Create a study, run benchmarks, upload results</li> <li>Destroy infrastructure (always, even on failure)</li> </ol> <p>The workflow is triggered by:</p> <ul> <li>Release publication on ArmoniK.Core</li> <li>Manual dispatch with a release tag input</li> <li>Repository dispatch webhook from external systems</li> </ul> <p>Results are uploaded as GitHub Actions artifacts and stored in S3.</p>"},{"location":"getting-started/#serving-the-docs-locally","title":"Serving the docs locally","text":"<pre><code>uv run --group docs microbenchmark.py dev serve-docs\n</code></pre> <p>This starts a local MkDocs server at <code>http://127.0.0.1:8000</code>.</p>"},{"location":"study/","title":"Studies","text":"<p>A study is the central organizational unit in ArmoniK.Microbench. It groups together everything related to a benchmarking session: which version of ArmoniK.Core was tested, the benchmark configurations used, where results are stored, and metadata about each run.</p>"},{"location":"study/#concept","title":"Concept","text":"<p>Studies are stored as JSON files in the <code>studies/</code> directory (gitignored by default). Each study can contain multiple runs -- for example, you might run the same study against different infrastructure configurations or after a code change to compare results.</p> <pre><code>studies/\n  release-0.25.1-20260215-143022.json\n  redis-scaling-test.json\n  queue-comparison.json\n</code></pre>"},{"location":"study/#study-json-structure","title":"Study JSON Structure","text":"<pre><code>{\n    \"name\": \"release-0.25.1-20260215-143022\",\n    \"core_version\": \"0.25.1\",\n    \"benchmark_runner_version\": \"latest\",\n    \"creation_date\": \"2026-02-15T14:30:22.123456\",\n    \"shared_private_key_path\": \"./infrastructure/generated/benchmark_key.pem\",\n    \"runs\": [\n        {\n            \"runner_config\": \"./infrastructure/benchmark_configs/runners/benchmark_runner.json\",\n            \"runner_config_contents\": { \"host\": \"...\", \"key\": \"...\" },\n            \"date\": \"2026-02-15T14:35:00.000000\",\n            \"benchmarks\": {\n                \"redis.json\": {\n                    \"source\": \"{ ... config contents ... }\",\n                    \"results\": \"s3://armonik-microbench-results/release-0.25.1/.../results.zip\",\n                    \"logs\": \"s3://armonik-microbench-results/release-0.25.1/.../logs.txt\",\n                    \"status\": \"success\"\n                },\n                \"sqs.json\": {\n                    \"source\": \"{ ... config contents ... }\",\n                    \"results\": \"s3://armonik-microbench-results/release-0.25.1/.../results.zip\",\n                    \"logs\": \"s3://armonik-microbench-results/release-0.25.1/.../logs.txt\",\n                    \"status\": \"success\"\n                }\n            }\n        }\n    ],\n    \"additional_notes\": \"\"\n}\n</code></pre> <p>Key fields:</p> <ul> <li>core_version -- The ArmoniK.Core tag/branch checked out on the runner for this study</li> <li>runs -- A list of run entries. Each run contains the runner config snapshot and a map of benchmark results</li> <li>benchmarks[name].source -- A snapshot of the benchmark config file contents at the time of the run (for reproducibility)</li> <li>benchmarks[name].results -- S3 URI pointing to the zipped BenchmarkDotNet artifacts</li> <li>benchmarks[name].logs -- S3 URI pointing to the full console output log</li> </ul>"},{"location":"study/#cli-reference","title":"CLI Reference","text":"<p>All study commands are under the <code>study</code> subcommand group:</p> <pre><code>uv run microbenchmark.py study &lt;command&gt; [options]\n</code></pre>"},{"location":"study/#study-create","title":"<code>study create</code>","text":"<p>Create a new study.</p> <pre><code>uv run microbenchmark.py study create &lt;STUDY_NAME&gt; [OPTIONS]\n</code></pre> Option Default Description <code>--core-version</code> <code>latest</code> ArmoniK.Core version/tag to benchmark against <code>--runner-version</code> <code>latest</code> Benchmark runner version <code>--key-path</code> <code>./infrastructure/generated/benchmark_key.pem</code> Path to the SSH private key <p>Example:</p> <pre><code>uv run microbenchmark.py study create \"redis-perf-test\" --core-version \"0.25.1\"\n</code></pre>"},{"location":"study/#study-run","title":"<code>study run</code>","text":"<p>Run benchmarks within a study. This is the main command that orchestrates the full benchmark lifecycle on the remote runner.</p> <pre><code>uv run microbenchmark.py study run &lt;STUDY_NAME&gt; [OPTIONS]\n</code></pre> Option Default Description <code>--runner</code> <code>./infrastructure/benchmark_configs/runners/benchmark_runner.json</code> Path to the runner config file (contains host and key) <code>-c</code>, <code>--config</code> -- Path to an individual benchmark config file. Can be specified multiple times <code>--directory</code> -- Path to a directory of benchmark config files (<code>.json</code>, <code>.yaml</code>, <code>.yml</code>) <code>--s3-bucket</code> <code>armonik-microbench-results</code> S3 bucket for storing results <code>--profile</code> <code>default</code> (or <code>$AWS_PROFILE</code>) AWS profile to use <code>--repo-url</code> <code>https://github.com/aneoconsulting/ArmoniK.Microbench.git</code> Repository URL to clone on the runner <code>--repo-branch</code> <code>main</code> Branch to checkout <code>--skip-init</code> <code>false</code> Skip the initialization step (clone + restore) <code>--skip-build</code> <code>false</code> Skip the ArmoniK.Core build step <p>Note</p> <p>You must provide at least one of <code>--config</code> or <code>--directory</code>.</p> <p>What <code>study run</code> does under the hood:</p> <ol> <li>Init (unless <code>--skip-init</code>): SSHs into the runner, clones the repo, and runs <code>dotnet restore</code></li> <li>Build (unless <code>--skip-build</code>): Checks out the ArmoniK.Core version from the study, runs <code>dotnet build -c Release</code></li> <li>Benchmark: For each config file, uploads it to the runner, executes BenchmoniK, uploads results and logs to S3</li> <li>Record: Saves the run entry (configs, S3 URIs, status) into the study JSON</li> </ol> <p>Examples:</p> <pre><code># Run all configs in a directory\nuv run microbenchmark.py study run \"my-study\" \\\n  --directory \"./infrastructure/benchmark_configs\"\n\n# Run specific config files\nuv run microbenchmark.py study run \"my-study\" \\\n  -c \"./infrastructure/benchmark_configs/redis.json\" \\\n  -c \"./infrastructure/benchmark_configs/sqs.json\"\n\n# Re-run without re-initializing (useful for iterating)\nuv run microbenchmark.py study run \"my-study\" \\\n  --skip-init --skip-build \\\n  -c \"./infrastructure/benchmark_configs/redis.json\"\n</code></pre>"},{"location":"study/#study-sync","title":"<code>study sync</code>","text":"<p>Download study results from S3 to a local directory.</p> <pre><code>uv run microbenchmark.py study sync &lt;STUDY_NAME&gt; [OPTIONS]\n</code></pre> Option Default Description <code>--output-dir</code> <code>./results</code> Local directory to download results into <code>--profile</code> <code>default</code> (or <code>$AWS_PROFILE</code>) AWS profile to use <code>--no-profile</code> <code>false</code> Use default credential chain instead of a named profile <code>--run-index</code> all runs Sync only a specific run by index <p>Output structure:</p> <pre><code>results/\n  my-study/\n    run_0_2026-02-15/\n      redis/\n        config.json      # Snapshot of the benchmark config\n        results.zip      # BenchmarkDotNet artifacts\n        logs.txt         # Full console output\n      sqs/\n        config.json\n        results.zip\n        logs.txt\n</code></pre> <p>Examples:</p> <pre><code># Sync all runs\nuv run microbenchmark.py study sync \"my-study\"\n\n# Sync only the latest run\nuv run microbenchmark.py study sync \"my-study\" --run-index 0\n\n# Use default AWS credential chain (e.g. on EC2 with instance profile)\nuv run microbenchmark.py study sync \"my-study\" --no-profile\n</code></pre>"},{"location":"study/#runner-commands","title":"Runner Commands","text":"<p>For lower-level control, you can use the <code>runner</code> commands directly. These operate on the remote runner instance without the study abstraction.</p>"},{"location":"study/#runner-init","title":"<code>runner init</code>","text":"<p>Initialize the benchmark runner instance (clone repo, restore .NET dependencies).</p> <pre><code>uv run microbenchmark.py runner init [OPTIONS]\n</code></pre> Option Default Description <code>--host</code> (from runner config) Hostname/IP of the runner <code>--key</code> (from runner config) Path to PEM key file <code>--repo-url</code> <code>https://github.com/aneoconsulting/ArmoniK.Microbench.git</code> Repository to clone <code>--repo-branch</code> <code>main</code> Branch to checkout <p>If <code>--host</code> and <code>--key</code> are not provided, the command reads them from <code>./infrastructure/benchmark_configs/runners/benchmark_runner.json</code> (generated by Terraform).</p>"},{"location":"study/#runner-build-core","title":"<code>runner build-core</code>","text":"<p>Build a specific ArmoniK.Core version on the runner.</p> <pre><code>uv run microbenchmark.py runner build-core [OPTIONS]\n</code></pre> Option Default Description <code>--host</code> (from runner config) Hostname/IP of the runner <code>--key</code> (from runner config) Path to PEM key file <code>--repo-branch</code> <code>main</code> ArmoniK.Core tag/branch to checkout and build"},{"location":"study/#runner-bench","title":"<code>runner bench</code>","text":"<p>Run a benchmark (or set of benchmarks) on the runner.</p> <pre><code>uv run microbenchmark.py runner bench [OPTIONS]\n</code></pre> Option Default Description <code>--host</code> (from runner config) Hostname/IP of the runner <code>--key</code> (from runner config) Path to PEM key file <code>--config-file</code>, <code>-c</code> -- Path to a single benchmark config file <code>--config-dir</code>, <code>-d</code> -- Path to a directory of config files"},{"location":"study/#runner-retrieve-results","title":"<code>runner retrieve-results</code>","text":"<p>Retrieve benchmark results from the runner, upload to S3, and download locally.</p> <pre><code>uv run microbenchmark.py runner retrieve-results [OPTIONS]\n</code></pre> Option Default Description <code>--host</code> (from runner config) Hostname/IP of the runner <code>--key</code> (from runner config) Path to PEM key file <code>--s3-bucket</code> <code>armonik-microbench-results</code> S3 bucket for results <code>--s3-key</code> <code>benchmark-artifacts.zip</code> S3 key for the uploaded archive <code>--profile</code> <code>default</code> (or <code>$AWS_PROFILE</code>) AWS profile <code>--output-dir</code> <code>.</code> Local directory to download results to"},{"location":"study/#dev-commands","title":"Dev Commands","text":"<p>Utility commands for development:</p>"},{"location":"study/#dev-serve-docs","title":"<code>dev serve-docs</code>","text":"<p>Serve the MkDocs documentation locally.</p> <pre><code>uv run microbenchmark.py dev serve-docs [--port 8000]\n</code></pre>"},{"location":"study/#dev-publish-docs","title":"<code>dev publish-docs</code>","text":"<p>Build and deploy documentation to GitHub Pages.</p> <pre><code>uv run microbenchmark.py dev publish-docs [--message \"Update docs\"] [--force]\n</code></pre>"},{"location":"components/infrastructure/","title":"Infrastructure","text":"<p>The <code>infrastructure/</code> directory contains modular Terraform code for deploying all AWS resources needed for benchmarking. Each ArmoniK adapter has its own Terraform module that provisions the corresponding AWS service and outputs a benchmark configuration JSON file.</p>"},{"location":"components/infrastructure/#architecture-overview","title":"Architecture Overview","text":"<pre><code>                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502                   VPC                       \u2502\n                        \u2502                                             \u2502\n                        \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n                        \u2502  \u2502          Public Subnets                \u2502  \u2502\n                        \u2502  \u2502                                       \u2502  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   SSH      \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502  \u2502\n\u2502 CLI /    \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502  \u2502  \u2502  EC2 Runner   \u2502                    \u2502  \u2502\n\u2502 CI Runner\u2502            \u2502  \u2502  \u2502  (.NET, AWS)  \u2502                    \u2502  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2502  \u2502\n                        \u2502  \u2502         \u2502                             \u2502  \u2502\n                        \u2502  \u2502    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502\n                        \u2502  \u2502    \u2502         \u2502          \u2502         \u2502  \u2502  \u2502\n                        \u2502  \u2502    v         v          v         v  \u2502  \u2502\n                        \u2502  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2510\u2502  \u2502\n                        \u2502  \u2502 \u2502Redis \u2502 \u2502Amazon\u2502 \u2502RabbitMQ\u2502 \u2502EFS \u2502\u2502  \u2502\n                        \u2502  \u2502 \u2502Cache \u2502 \u2502  MQ  \u2502 \u2502  EC2   \u2502 \u2502    \u2502\u2502  \u2502\n                        \u2502  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2518\u2502  \u2502\n                        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                         \u2502\n                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                         \u2502               \u2502               \u2502\n                         v               v               v\n                      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                      \u2502  S3  \u2502      \u2502  SQS   \u2502     \u2502 Results \u2502\n                      \u2502Bucket\u2502      \u2502 Queues \u2502     \u2502   S3    \u2502\n                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>All resources are created within a dedicated VPC. The EC2 runner instance communicates with each service over the private network. S3 and SQS are accessed via public AWS endpoints from the runner's IAM role.</p>"},{"location":"components/infrastructure/#how-modules-work","title":"How Modules Work","text":"<p>Each module follows the same pattern:</p> <ol> <li>Provisions AWS resources (the service itself, security groups, IAM policies)</li> <li>Outputs a benchmark config JSON file to <code>infrastructure/benchmark_configs/</code> -- this file is consumed by BenchmoniK at runtime</li> <li>Wires security groups to allow the runner instance to communicate with the service</li> </ol> <p>Modules are conditionally deployed using the <code>count</code> meta-argument. Setting a variable to <code>null</code> (or omitting it) disables the module entirely:</p> <pre><code># In parameters.tfvars:\nredis_benchmark = { instance_type = \"cache.m5.xlarge\" }  # Deployed\ns3_benchmark    = {}                                      # Deployed with defaults\nsqs_benchmark   = null                                    # Not deployed\n# efs_benchmark                                           # Omitted = not deployed\n</code></pre>"},{"location":"components/infrastructure/#module-reference","title":"Module Reference","text":""},{"location":"components/infrastructure/#runner-modulesrunner","title":"Runner (<code>modules/runner/</code>)","text":"<p>The runner module is always deployed. It creates the EC2 instance where benchmarks execute.</p> <p>Resources created:</p> <ul> <li>EC2 instance (Ubuntu 24.04) with .NET SDK 8.0 + 10.0 and AWS CLI pre-installed</li> <li>IAM role + instance profile with S3 access to the results bucket</li> <li>Security group (SSH ingress, all egress)</li> </ul> <p>Variables:</p> Variable Type Default Description <code>instance_type</code> string <code>t2.micro</code> EC2 instance type. Use <code>c7a.8xlarge</code> or similar for real benchmarks <code>volume_type</code> string <code>gp3</code> EBS volume type <code>ssh_key_name</code> string (required) AWS key pair name for SSH access <code>benchmark_results_bucket_name</code> string (required) S3 bucket name for uploading results <code>efs_mount_target_ip</code> string <code>\"\"</code> EFS mount target IP. If set, the instance mounts EFS at <code>/mnt/efs</code> <code>network_config</code> object (required) <code>{ vpc_id, subnet_id }</code> <p>Config output: <code>benchmark_configs/runners/benchmark_runner.json</code></p> <pre><code>{\n  \"host\": \"ec2-xx-xx-xx-xx.compute-1.amazonaws.com\",\n  \"key\": \"/absolute/path/to/generated/benchmark_key.pem\",\n  \"ResourceMetadata\": {\n    \"Arn\": \"arn:aws:ec2:...\",\n    \"ResourceId\": \"i-xxxx\",\n    \"NodeType\": \"c7a.8xlarge\",\n    \"VolumeType\": \"gp3\"\n  }\n}\n</code></pre>"},{"location":"components/infrastructure/#redis-modulesredis","title":"Redis (<code>modules/redis/</code>)","text":"<p>Deploys an AWS ElastiCache Redis cluster for benchmarking the Redis object storage adapter.</p> <p>Resources created:</p> <ul> <li>ElastiCache Redis cluster (single node)</li> <li>ElastiCache subnet group</li> <li>Security group (port 6379)</li> </ul> <p>Variables:</p> Variable Type Default Description <code>node_type</code> string <code>cache.t3.micro</code> ElastiCache node type <code>engine_version</code> string <code>7.0</code> Redis engine version <code>param_group_name</code> string <code>default.redis7</code> Parameter group name <code>network_config</code> object (required) <code>{ vpc_id, subnet_id, subnet_ids }</code> <p>Config output: <code>benchmark_configs/redis.json</code></p> <pre><code>{\n  \"Component\": \"Redis\",\n  \"Redis:EndpointUrl\": \"xxx.cache.amazonaws.com:6379\",\n  \"Redis:ClientName\": \"BenchmoniK\",\n  \"Redis:InstanceName\": \"benchmark\",\n  \"Redis:MaxRetry\": 5,\n  \"Redis:MsAfterRetry\": 500,\n  \"Redis:Timeout\": 5000,\n  \"Redis:Ssl\": false,\n  \"ResourceMetadata\": { ... }\n}\n</code></pre>"},{"location":"components/infrastructure/#s3-moduless3","title":"S3 (<code>modules/s3/</code>)","text":"<p>Creates an S3 bucket for benchmarking the S3 object storage adapter.</p> <p>Resources created:</p> <ul> <li>S3 bucket (with random suffix for uniqueness)</li> <li>IAM policy granting the runner role access to the bucket</li> </ul> <p>Variables:</p> Variable Type Default Description <code>benchmark_runner_role_id</code> string (required) IAM role ID of the runner (for policy attachment) <code>additional_s3_config</code> map <code>{}</code> Extra key-value pairs merged into the config output <p>Config output: <code>benchmark_configs/s3.json</code></p> <pre><code>{\n  \"Component\": \"S3\",\n  \"S3:BucketName\": \"prefix-s3-benchmark-xxxx\",\n  \"S3:EndpointUrl\": \"https://s3.us-east-1.amazonaws.com\",\n  \"S3:Region\": \"us-east-1\",\n  \"S3:Profile\": \"default\",\n  \"S3:MustForcePathStyle\": true,\n  \"ResourceMetadata\": { ... }\n}\n</code></pre>"},{"location":"components/infrastructure/#localstorage-local-fs-moduleslocalstoragelocalfs","title":"LocalStorage - Local FS (<code>modules/localstorage/localfs/</code>)","text":"<p>The simplest module. It just generates a config file pointing BenchmoniK at a local filesystem path on the runner. No AWS resources are created.</p> <p>Variables:</p> Variable Type Default Description <code>storage_path</code> string (required) Filesystem path on the runner to use for storage <p>Config output: <code>benchmark_configs/localstorage.json</code></p> <pre><code>{\n  \"Component\": \"LocalStorage\",\n  \"LocalStorage:Path\": \"/tmp/localstorage_benchtemp\"\n}\n</code></pre>"},{"location":"components/infrastructure/#localstorage-efs-moduleslocalstorageefs","title":"LocalStorage - EFS (<code>modules/localstorage/efs/</code>)","text":"<p>Deploys an AWS EFS filesystem for benchmarking the LocalStorage adapter over a network filesystem.</p> <p>Resources created:</p> <ul> <li>EFS file system (generalPurpose performance mode)</li> <li>Security group (NFS port 2049, restricted to the runner SG)</li> <li>EFS mount target in the benchmark subnet</li> </ul> <p>Variables:</p> Variable Type Default Description <code>instance_security_group_id</code> string (required) Runner's security group ID (for NFS ingress rule) <code>network_config</code> object (required) <code>{ vpc_id, subnet_id }</code> <p>Config output: <code>benchmark_configs/efs.json</code></p> <pre><code>{\n  \"Component\": \"LocalStorage\",\n  \"LocalStorage:Path\": \"/mnt/efs\"\n}\n</code></pre> <p>Note</p> <p>The EFS mount target IP must be passed to the runner module's <code>efs_mount_target_ip</code> variable so that the runner's user data script mounts the filesystem at <code>/mnt/efs</code>.</p>"},{"location":"components/infrastructure/#sqs-modulessqs","title":"SQS (<code>modules/sqs/</code>)","text":"<p>Sets up IAM permissions for the runner to create and manage SQS queues. SQS queues are created dynamically by the benchmark itself -- this module only grants the necessary permissions.</p> <p>Resources created:</p> <ul> <li>IAM policy with SQS permissions (scoped to <code>prefix*</code> queue names)</li> <li>Policy attachment to the runner IAM role</li> </ul> <p>Variables:</p> Variable Type Default Description <code>benchmark_runner_role_id</code> string (required) IAM role ID of the runner <p>Config output: <code>benchmark_configs/sqs.json</code></p> <pre><code>{\n  \"Component\": \"SQS\",\n  \"SQS:ServiceURL\": \"https://sqs.us-east-1.amazonaws.com\",\n  \"SQS:Prefix\": \"prefix\"\n}\n</code></pre>"},{"location":"components/infrastructure/#amazonmq-modulesamazonmq","title":"AmazonMQ (<code>modules/amazonmq/</code>)","text":"<p>Deploys an AWS-managed message broker via AmazonMQ. This module supports both RabbitMQ and ActiveMQ -- the engine type is controlled by the <code>engine_type</code> variable.</p> <p>Resources created:</p> <ul> <li>AmazonMQ broker (single-instance deployment)</li> <li>Security group with AMQP (5671) and management console (8162) ports</li> <li>Security group rules for runner-to-broker communication</li> <li>Random credentials (if not overridden)</li> <li>ActiveMQ XML configuration (ActiveMQ engine only)</li> </ul> <p>Variables:</p> Variable Type Default Description <code>engine_type</code> string <code>RabbitMQ</code> <code>RabbitMQ</code> or <code>ActiveMQ</code> <code>engine_version</code> string <code>3.13</code> Engine version <code>host_instance_type</code> string <code>mq.m5.xlarge</code> Broker instance type <code>mq_username_override</code> string (random) Override the auto-generated username <code>mq_password_override</code> string (random) Override the auto-generated password <code>benchmark_runner_sg_id</code> string (required) Runner's security group ID <code>network_config</code> object (required) <code>{ vpc_id, subnet_id }</code> <p>Config output: <code>benchmark_configs/rabbitmq-amq.json</code> or <code>benchmark_configs/activemq.json</code></p> <pre><code>{\n  \"Component\": \"RabbitMQ\",\n  \"Amqp:Host\": \"b-xxxx.mq.us-east-1.amazonaws.com\",\n  \"Amqp:Port\": 5671,\n  \"Amqp:Scheme\": \"AMQPS\",\n  \"Amqp:User\": \"rabbitmqbench\",\n  \"Amqp:Password\": \"rabbitmqbench\",\n  \"Amqp:MaxRetries\": 1,\n  \"ResourceMetadata\": { ... }\n}\n</code></pre> <p>In <code>main.tf</code>, this module is instantiated twice (once for RabbitMQ, once for ActiveMQ):</p> <pre><code>module \"rabbitmq_amq\" {\n  source      = \"./modules/amazonmq\"\n  engine_type = \"RabbitMQ\"       # defaults\n  # ...\n}\n\nmodule \"activemq\" {\n  source         = \"./modules/amazonmq\"\n  engine_type    = \"ActiveMQ\"\n  engine_version = \"5.18\"\n  # ...\n}\n</code></pre>"},{"location":"components/infrastructure/#rabbitmq-ec2-modulesrabbitmqec2","title":"RabbitMQ EC2 (<code>modules/rabbitmq/ec2/</code>)","text":"<p>Deploys RabbitMQ on a standalone EC2 instance. This is an alternative to the AmazonMQ-managed approach, useful when you need more control over the RabbitMQ configuration or want to test unmanaged deployments.</p> <p>Resources created:</p> <ul> <li>EC2 instance (Ubuntu 22.04) with RabbitMQ 4.x installed via user data script</li> <li>Security group (AMQP 5672, management UI 15672)</li> <li>Random credentials (if not overridden)</li> <li>Optional CloudWatch log group and SSM parameter for log shipping</li> </ul> <p>Variables:</p> Variable Type Default Description <code>instance_type</code> string <code>m5.4xlarge</code> EC2 instance type <code>rabbitmq_username</code> string (random) Override the auto-generated username <code>rabbitmq_password</code> string (random) Override the auto-generated password <code>enable_cloudwatch_logs</code> bool <code>false</code> Enable CloudWatch log shipping <code>network_config</code> object (required) <code>{ vpc_id, subnet_id }</code> <p>Config output: <code>benchmark_configs/rabbitmq-ec2.json</code></p> <pre><code>{\n  \"Component\": \"RabbitMQ\",\n  \"Amqp:Host\": \"10.0.1.xx\",\n  \"Amqp:Port\": 5672,\n  \"Amqp:Scheme\": \"AMQP\",\n  \"Amqp:User\": \"xxxx\",\n  \"Amqp:Password\": \"xxxx\",\n  \"Amqp:MaxRetries\": 1,\n  \"ResourceMetadata\": { ... },\n  \"Management\": {\n    \"Host\": \"ec2-xx-xx-xx-xx.compute-1.amazonaws.com\",\n    \"IP\": \"xx.xx.xx.xx\",\n    \"Port\": 15672\n  }\n}\n</code></pre>"},{"location":"components/infrastructure/#shared-resources","title":"Shared Resources","text":""},{"location":"components/infrastructure/#vpc","title":"VPC","text":"<p>Created in <code>main.tf</code> using the <code>terraform-aws-modules/vpc/aws</code> community module:</p> <ul> <li>CIDR: <code>10.0.0.0/16</code></li> <li>Two public subnets (<code>10.0.1.0/24</code>, <code>10.0.2.0/24</code>)</li> <li>DNS hostnames and support enabled</li> </ul>"},{"location":"components/infrastructure/#ssh-key-pair","title":"SSH Key Pair","text":"<p>Generated by Terraform using the <code>tls_private_key</code> resource (RSA 2048-bit). The private key is written to <code>infrastructure/generated/benchmark_key.pem</code> and also available as a Terraform output.</p>"},{"location":"components/infrastructure/#common-tags","title":"Common Tags","text":"<p>All resources are tagged with:</p> <pre><code>{\n  application          = \"Microbenchmarks\"\n  \"deployment version\" = \"${prefix}-armonik-microbench\"\n}\n</code></pre> <p>Plus per-module tags identifying the module name.</p>"},{"location":"components/infrastructure/#terraform-state","title":"Terraform State","text":"<p>For CI/CD runs, the Terraform state is stored remotely in an S3 bucket (<code>armonik-microbench-backend-tfstate</code>) with a run-specific key:</p> <pre><code>s3://armonik-microbench-backend-tfstate/microbench/&lt;run-id&gt;/terraform.tfstate\n</code></pre> <p>For local development, state defaults to the local filesystem (<code>terraform.tfstate</code>).</p>"},{"location":"components/infrastructure/#config-file-flow","title":"Config File Flow","text":"<pre><code>terraform apply\n    \u2502\n    \u251c\u2500\u2500 modules/runner/outputs.tf   \u2192 benchmark_configs/runners/benchmark_runner.json\n    \u251c\u2500\u2500 modules/redis/outputs.tf    \u2192 benchmark_configs/redis.json\n    \u251c\u2500\u2500 modules/s3/outputs.tf       \u2192 benchmark_configs/s3.json\n    \u251c\u2500\u2500 modules/sqs/outputs.tf      \u2192 benchmark_configs/sqs.json\n    \u251c\u2500\u2500 modules/amazonmq/outputs.tf \u2192 benchmark_configs/rabbitmq-amq.json (or activemq.json)\n    \u251c\u2500\u2500 modules/rabbitmq/ec2/...    \u2192 benchmark_configs/rabbitmq-ec2.json\n    \u251c\u2500\u2500 modules/localstorage/localfs \u2192 benchmark_configs/localstorage.json\n    \u2514\u2500\u2500 modules/localstorage/efs/... \u2192 benchmark_configs/efs.json\n         \u2502\n         v\n    microbenchmark.py study run --directory ./infrastructure/benchmark_configs\n         \u2502\n         v\n    BenchmoniK reads \"Component\" field \u2192 dispatches to correct benchmark class\n</code></pre> <p>The <code>benchmark_configs/</code> directory is gitignored since it contains deployment-specific values (endpoints, credentials).</p>"}]}